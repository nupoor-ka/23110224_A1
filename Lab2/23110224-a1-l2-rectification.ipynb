{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12937598,"sourceType":"datasetVersion","datasetId":8186901}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T04:49:41.158939Z","iopub.execute_input":"2025-09-03T04:49:41.159094Z","iopub.status.idle":"2025-09-03T04:49:57.323841Z","shell.execute_reply.started":"2025-09-03T04:49:41.159079Z","shell.execute_reply":"2025-09-03T04:49:57.322937Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nCollecting transformers\n  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m833.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.33.1\n    Uninstalling huggingface-hub-0.33.1:\n      Successfully uninstalled huggingface-hub-0.33.1\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.4\n    Uninstalling transformers-4.52.4:\n      Successfully uninstalled transformers-4.52.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.34.4 tokenizers-0.22.0 transformers-4.56.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import csv\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom IPython.display import FileLink\nimport re\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:15:49.552479Z","iopub.execute_input":"2025-09-03T05:15:49.553124Z","iopub.status.idle":"2025-09-03T05:15:49.556701Z","shell.execute_reply.started":"2025-09-03T05:15:49.553098Z","shell.execute_reply":"2025-09-03T05:15:49.556016Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\") # model for rectification\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\", torch_dtype=torch.float16, device_map=\"auto\").eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T04:50:06.353480Z","iopub.execute_input":"2025-09-03T04:50:06.353876Z","iopub.status.idle":"2025-09-03T04:51:26.372024Z","shell.execute_reply.started":"2025-09-03T04:50:06.353856Z","shell.execute_reply":"2025-09-03T04:51:26.371177Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3476b740bbb44f449848311111f9dad4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79efd2651f6b45b587598d0c96efe7aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e25013905e54853a7a6d8a50318250c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be6088cc13d49ba881828a756d1bf77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49ee2e61c9a24329aef5bbb2b2deff6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43bd618c1b55445694e8ff621387e924"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n2025-09-03 04:50:13.937518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756875014.139441      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756875014.196344      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"424ba8f178e84862a90d96a4b4cc48c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe77acae0dd419a8bcde50dbc31cd07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"136d50109a5c4ea39f86b83a2e672b28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8c2b49af6c34780ab5e8156b522a0e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a55634dcafa4831aeb79f80bfe6885b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74c3cc5aa28742e596f6780bafb481fa"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# this is an instruct model, works better with the chat functionality, heavier than commit model, but hopefully better\ndef build_prompt(message: str, filename: str, inference: str) -> str: # wasn't working in single shot, need to do this\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are a senior engineer. Rewrite commit messages as a single, \"\n                \"concise, imperative one-liner (<= 80 chars). \"\n                \"Do NOT include explanations, prefixes, or extra lines.\"\n            ),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"Example:\\n\"\n                \"Message: fixed bug in login function\\n\"\n                \"Filename: auth.py\\n\"\n                \"Fix type: bugfix\\n\"\n                \"Output: fix login bug in auth.py\\n\\n\"\n                f\"Now process:\\n\"\n                f\"Message: {message}\\n\"\n                f\"Filename: {filename}\\n\"\n                f\"Fix type: {inference}\\n\"\n                \"Output ONLY the final commit message.\"\n            ),\n        },\n    ]\n    return tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\ndef postprocess(text: str) -> str:\n    t = text.strip()\n    t = re.sub(r'(?i)^create a (new )?commit message.*?:\\s*', '', t).strip() # it was echoing this sentence hence\n    for line in t.splitlines(): # get only first non empty line, it was giving multiple lines, with repetition hence\n        if line.strip():\n            return line.strip()\n    return t\n\n@torch.inference_mode()\ndef rectify_commit(message: str, filename: str, inference: str) -> str:\n    prompt = build_prompt(message, filename, inference)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device) # jic, because gpu\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=64,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3, # don't repeat, was doing this earlier\n        repetition_penalty=1.05, # same as above\n        eos_token_id=tokenizer.eos_token_id,\n    )\n\n    gen_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]  # cut off prompt\n    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n    return postprocess(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:15:29.932491Z","iopub.execute_input":"2025-09-03T05:15:29.933040Z","iopub.status.idle":"2025-09-03T05:15:29.940059Z","shell.execute_reply.started":"2025-09-03T05:15:29.933015Z","shell.execute_reply":"2025-09-03T05:15:29.939252Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"input_file = \"/kaggle/input/repository-commit-data/input_for_rectification.csv\"\noutput_file = \"rectified_commits.csv\"\n\ncount = 0  # total rows 513\n\nwith open(input_file, 'r', encoding=\"utf-8\") as f_in, \\\n     open(output_file, 'w', newline='', encoding=\"utf-8\") as f_out:\n    \n    reader = csv.DictReader(f_in)\n    writer = csv.writer(f_out)\n    \n    writer.writerow(reader.fieldnames + ['Rectified Message']) # add col rectified msg\n    \n    for row in reader:\n        rect_msg = rectify_commit(message=row['Message'], filename=row['Filename'], inference=row['LLM Inference (fix type)'])\n        \n        writer.writerow([row[col] for col in reader.fieldnames] + [rect_msg])\n\n        count += 1\n        if count % 10 == 0:\n            print(f\"Row {count}:\")\n            print(f\"Message: {row['Message']}\\nFilename: {row['Filename']}\\nLLM Inference: {row['LLM Inference (fix type)']}\")\n            print(f\"Rectified message: {rect_msg}\\n\")\n\n        torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:15:55.479494Z","iopub.execute_input":"2025-09-03T05:15:55.480329Z","iopub.status.idle":"2025-09-03T05:39:12.361844Z","shell.execute_reply.started":"2025-09-03T05:15:55.480294Z","shell.execute_reply":"2025-09-03T05:39:12.361265Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Row 10:\nMessage: signal_stoploss_u_b_1: add Elder Ray Index check\nFilename: NostalgiaForInfinityNext.py\nLLM Inference: add nostalgia for infinity next\nRectified message: add elder ray index check in nostalgic infinity next\n\nRow 20:\nMessage: Also try `resolve()`'ed path. Log that `hold-trades.json` was not found\nFilename: NostalgiaForInfinityNext.py\nLLM Inference: fix nostalgiaforinfinitynext\nRectified message: fix resolve path, log hold trades not found\n\nRow 30:\nMessage: The exchange data now comes from https://github.com/iterativv/NostalgiaForInfinityData\nFilename: user_data\\backtesting-kucoin.env\nLLM Inference: update exchange_data.rb\nRectified message: update exchange data in backtesting.env\n\nRow 40:\nMessage: Add Emulated Backtest AgeFilter logic. Update exchange downtime protection (data validity).\nFilename: NostalgiaForInfinityNext.py\nLLM Inference: update nostalgiaforinfinitynext.py\nRectified message: add emulated backtest age filter and update exchange data validity\n\nRow 50:\nMessage: Rework the ATR based stoplosses.\nFilename: NostalgiaForInfinityNext.py\nLLM Inference: update nostalgiaforinfinitynext.py\nRectified message: rework ATR stoploss in NFIN.py\n\nRow 60:\nMessage: Fix `.startswith()` not expecting a `set`\nFilename: .github\\workflows\\scripts\\comment-ci-results.py\nLLM Inference: fix comment handling for github actions bot\nRectified message: fix .startswith() bug in comment-ci results workflow\n\nRow 70:\nMessage: Fix typo.\nFilename: NostalgiaForInfinityNext.py\nLLM Inference: fix typo in pmax\nRectified message: fix pmax typo in NFI.py\n\nRow 80:\nMessage: Fix. Typo in pivot protection\nFilename: NostalgiaForInfinityNext.py\nLLM Inference: fix nostalgiaforinfinitynext\nRectified message: fix typo in pivot protect NFIN\n\nRow 90:\nMessage: Add all quick mode buy signals to extra stoploss.\nFilename: NostalgiaForInfinityNext.py\nLLM Inference: update nostalgiaforinfinitynext.py\nRectified message: add quick mode signals to stoploss in extra innostalgia for infinity next\n\nRow 100:\nMessage: Add binance deviations  I want to change some buy conditions to get more good trades but it's (almost) impossible to keep the drawdown below 30%.\nFilename: tests\\backtests\\test_winrate_and_drawdown.py\nLLM Inference: update binance deviations.py\nRectified message: adjust binance devs.py for better trades with <30% drawdown\n\nRow 110:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityNextGen.py\nLLM Inference: update nostalgia for infinity next gen\nRectified message: rework sell-stoploss in NFINNG.py\n\nRow 120:\nMessage: sell_pump_stoploss: stoploss for pumped pairs.\nFilename: NostalgiaForInfinityNextGen.py\nLLM Inference: update sell_pump_stoploss.py\nRectified message: fix sell pump stoploss update in NostoGiaFoInFiNyNextGen\n\nRow 130:\nMessage: sell_pump_stoploss: rework.\nFilename: NostalgiaForInfinityNextGen.py\nLLM Inference: update semantic_pump_stoploss.py\nRectified message: rework semantic pump stoploss\n\nRow 140:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityNextGen.py\nLLM Inference: update nostalgiaforinfinitynextgen.py\nRectified message: rework sell-stoploss in NFINNG\n\nRow 150:\nMessage: Fix naming for X.\nFilename: tests\\backtests\\helpers.py\nLLM Inference: fix naming for x. diff\nRectified message: fix x naming in tests/backtests/helpers\n\nRow 160:\nMessage: Fine tune ATR stoploss thresholds.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update nostalgiaforinfinityx.py\nRectified message: fine-tune atr stoploss in NFix.py\n\nRow 170:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update nostalgiaforinfinityx.py\nRectified message: rework sell-stoploss in NFix.py\n\nRow 180:\nMessage: Don't use the FT stoploss.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: fix nostalgiaforinfinityx\nRectified message: fix stoploss FT in NFix\n\nRow 190:\nMessage: Fix docs for hold trades.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update hold_trades.json\nRectified message: fix docs for holding trades in nostalgia_for_infinity_x.py\n\nRow 200:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update minimal_roi.py\nRectified message: rework sell-stoploss in NFI.py, update min-roi in min-ROI.py\n\nRow 210:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update minimal_roi.py\nRectified message: rework sell-stoploss in NFI.py, update min-roi in min-ROI.py\n\nRow 220:\nMessage: Fix filename for CI backtests.\nFilename: tests\\backtests\\helpers.py\nLLM Inference: update backtest.py\nRectified message: fix backtest filename in tests/backtests/helpers\n\nRow 230:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update minimal_roi.py\nRectified message: rework sell-stoploss in NFI.py, update min-roi in min-ROI.py\n\nRow 240:\nMessage: Allow changing of the initial stake if rebuy is enabled and a fixed stake used.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update nodalgiaforinfinityx.py\nRectified message: update stake change on rebuys\n\nRow 250:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update nostalgiaforinfinityx.py\nRectified message: rework sell-stoploss in NFix.py\n\nRow 260:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update nostalgiaforinfinityx.py\nRectified message: rework sell-stoploss in NFix.py\n\nRow 270:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update minimal_roi.py\nRectified message: rework sell-stoploss in NFI.py, update min-roi in min-ROI.py\n\nRow 280:\nMessage: added separate stop loss for leverage tokens\nFilename: NostalgiaForInfinityX.py\nLLM Inference: add nostalgiaforinfinityx\nRectified message: add stop loss leverage token\n\nRow 290:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update minimal_roi.py\nRectified message: rework sell-stoploss in NFI.py, update min-roi in min-ROI.py\n\nRow 300:\nMessage: Fix compile issue due to S/R change\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update nostalgiaforinfinityx.py\nRectified message: fix s/r compile issue in nostalgiascript.py\n\nRow 310:\nMessage: Fix variable name.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: fix typo in nodalgiaforinfinityx\nRectified message: fix variable name typo in NostagiaForInfiniX\n\nRow 320:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update nostalgiaforinfinityx.py\nRectified message: rework sell-stoploss in NFix.py\n\nRow 330:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update nostalgiaforinfinityx.py\nRectified message: rework sell-stoploss in NFix.py\n\nRow 340:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update semantic_roi.py\nRectified message: rework sell stoploss in NFINX\n\nRow 350:\nMessage: Profit maximizer: turn on for the stoplosses.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: fix profit maximizer\nRectified message: activate profit_maximizer for stoploss settings in nostalgia_for_infinity_x.py\n\nRow 360:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update minimal_roi.py\nRectified message: rework sell-stoploss in NFI.py, update min-roi in min-ROI.py\n\nRow 370:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update roistrategy.py\nRectified message: rework sell stoploss strategy in NFI.py\n\nRow 380:\nMessage: Maximizer: allow sell_stoploss_stop_2 to use maximizer.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: enable profit maximizer for infinity x\nRectified message: enable profit_maximizer_infinity_x\n\nRow 390:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update roistrategy.py\nRectified message: rework sell stoploss strategy in NFI.py\n\nRow 400:\nMessage: sell_stoploss: normal stake to -16%.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update roistrategy.py\nRectified message: update sell stoploss strategy in nostalgiaforinfinityx.py\n\nRow 410:\nMessage: sell_stoploss: rework.\nFilename: NostalgiaForInfinityX.py\nLLM Inference: update nostalgiaforinfinityx.py\nRectified message: rework sell-stoploss in NFix.py\n\nRow 420:\nMessage: X2: exit_normal_bear_stoploss: rework.\nFilename: experimental\\NostalgiaForInfinityX2.py\nLLM Inference: fix typo in nodalgiaforinfinity\nRectified message: rework typo in experimental\\nostalgia for infinityX2\n\nRow 430:\nMessage: X2: fine tune the stop thresholds.\nFilename: NostalgiaForInfinityX2.py\nLLM Inference: update nostalgiaforinfinity.py\nRectified message: fine-tune X2's stop threshold\n\nRow 440:\nMessage: X2: exit_quick_bull_stoploss: rework.\nFilename: NostalgiaForInfinityX2.py\nLLM Inference: update exit_quick_bull_stoploss.py\nRectified message: fix exit quick bull stoploss rework in nostalgia_for_infinity_x2\n\nRow 450:\nMessage: X2: fine tune the stop settings.\nFilename: NostalgiaForInfinityX2.py\nLLM Inference: update nostalgiaforinfinity.py\nRectified message: fine-tune stop settings in NFix.py\n\nRow 460:\nMessage: X2: grinding: fix sell check.\nFilename: NostalgiaForInfinityX2.py\nLLM Inference: update minimal_roi.py\nRectified message: update sell check in NFX2\n\nRow 470:\nMessage: X2: change startup candles to 199 for bybit.\nFilename: NostalgiaForInfinityX2.py\nLLM Inference: change startup candles to 199 for bybit\nRectified message: change bybit startup candle to 200 in nostalgia for infinity x2\n\nRow 480:\nMessage: X3: fine tune stops.\nFilename: experimental\\NostalgiaForInfinityX3.py\nLLM Inference: infinity x3\nRectified message: fine-tune X3 stops in experimental/nostalgia_for_infinity_x3\n\nRow 490:\nMessage: X3: grinding: fine tune the stops.\nFilename: NostalgiaForInfinityX3.py\nLLM Inference: update minimal_roi.py\nRectified message: fine-tune X3 stops\n\nRow 500:\nMessage: Some fixups and improvements  new file:   .github/workflows/scripts/download-necessary-exchange-market-data-for-backtests.sh Added data downloader from DigiTuccar HistoricalDataForTradeBacktest repo https://github.com/DigiTuccar/HistoricalDataForTradeBacktest.git  modified:   .github/workflows/tests.yml Added new time periods and selective trading mode (futures and spot)  modified:   .gitignore added /user_data please check for your own configurations if necessary  deleted:    .gitmodules not needed  modified:   .pre-commit-config.yaml updated versions  modified:   README.md updated data downloading instructions  modified:   configs/exampleconfig-rebuy.json     \"position_adjustment_enable\": true, added  modified:   docker-compose.yml   Added      - \"./user_data/data:/testing/user_data/data\" for gitlab testing  modified:   docker/Dockerfile.custom   Added git-lfs support for testing in gitlab-ci  modified:   tests/backtests/helpers.py modified:   tests/backtests/test_winrate_and_drawdown.py modified:   tests/ci-requirements.txt modified:   tests/requirements.txt modified:   .github/workflows/scripts/comment-ci-results.py  modified:   tests/unit/conftest.py   Added NFIX3 for testing  new file:   tools/download-necessary-exchange-market-data-for-backtests.sh  modified:   user_data/strategies/NostalgiaForInfinityNext.py modified:   user_data/strategies/NostalgiaForInfinityNextGen.py   Updated file links to legacy\nFilename: .pre-commit-config.yaml\nLLM Inference: add links to legacy diff\nRectified message: update pre-commit config with legacy file links\n\nRow 510:\nMessage: Some fixups and improvements  new file:   .github/workflows/scripts/download-necessary-exchange-market-data-for-backtests.sh Added data downloader from DigiTuccar HistoricalDataForTradeBacktest repo https://github.com/DigiTuccar/HistoricalDataForTradeBacktest.git  modified:   .github/workflows/tests.yml Added new time periods and selective trading mode (futures and spot)  modified:   .gitignore added /user_data please check for your own configurations if necessary  deleted:    .gitmodules not needed  modified:   .pre-commit-config.yaml updated versions  modified:   README.md updated data downloading instructions  modified:   configs/exampleconfig-rebuy.json     \"position_adjustment_enable\": true, added  modified:   docker-compose.yml   Added      - \"./user_data/data:/testing/user_data/data\" for gitlab testing  modified:   docker/Dockerfile.custom   Added git-lfs support for testing in gitlab-ci  modified:   tests/backtests/helpers.py modified:   tests/backtests/test_winrate_and_drawdown.py modified:   tests/ci-requirements.txt modified:   tests/requirements.txt modified:   .github/workflows/scripts/comment-ci-results.py  modified:   tests/unit/conftest.py   Added NFIX3 for testing  new file:   tools/download-necessary-exchange-market-data-for-backtests.sh  modified:   user_data/strategies/NostalgiaForInfinityNext.py modified:   user_data/strategies/NostalgiaForInfinityNextGen.py   Updated file links to legacy\nFilename: tests\\unit\\conftest.py\nLLM Inference: add nfix to testdatadir\nRectified message: add NFIX to tests.ci-config in unit conftest\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"display(FileLink(\"rectified_commits.csv\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T06:02:28.892596Z","iopub.execute_input":"2025-09-03T06:02:28.892885Z","iopub.status.idle":"2025-09-03T06:02:28.898030Z","shell.execute_reply.started":"2025-09-03T06:02:28.892864Z","shell.execute_reply":"2025-09-03T06:02:28.897314Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/rectified_commits.csv","text/html":"<a href='rectified_commits.csv' target='_blank'>rectified_commits.csv</a><br>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# This didn't work, single shot makes it echo the input, repeat itself and not understand the task\n# input_file = \"/kaggle/input/repository-commit-data/input_for_rectification.csv\"\n# output_file = \"rectified_commits.csv\"\n\n# count = 0 # total 513\n\n# with open(input_file, 'r', encoding=\"utf-8\") as f_in, \\\n#      open(output_file, 'w', newline='', encoding=\"utf-8\") as f_out:\n    \n#     reader = csv.DictReader(f_in)\n#     writer = csv.writer(f_out)\n    \n#     # Write headers\n#     writer.writerow(reader.fieldnames + ['Rectified Message'])\n    \n#     for row in reader:\n#         prompt = f\"Create a new commit message, this is the information known:\\nMessage1: {row['Message']}\\nMessage 2: {row['LLM Inference (fix type)']}Filename: {row['Filename']}\"\n        \n#         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n#         output_ids = model.generate(input_ids, max_length=150, num_beams=5)\n#         rectified_message = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        \n#         writer.writerow([row[col] for col in reader.fieldnames] + [rectified_message])\n\n#         count+=1\n#         if count%10==0:\n#             print(f\"Row {count}:\")\n#             print(f\"Message: {row['Message']}\\nFilename: {row['Filename']}\\nLLM Inference: {row['LLM Inference (fix type)']}\")\n#             print(f\"Rectified message: {rectified_message}\\n\")\n        \n#         del input_ids, output_ids # clearing gpu memory jic\n#         torch.cuda.empty_cache()\n# display(FileName(\"rectified_commits.csv\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T06:02:26.297330Z","iopub.execute_input":"2025-09-03T06:02:26.297618Z","iopub.status.idle":"2025-09-03T06:02:26.301569Z","shell.execute_reply.started":"2025-09-03T06:02:26.297596Z","shell.execute_reply":"2025-09-03T06:02:26.300867Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":15}]}
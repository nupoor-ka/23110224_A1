{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb7fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import torch\n",
    "import os\n",
    "from pydriller import Repository\n",
    "import sys\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mamiksik/CommitPredictorT5\") # model for commit creation\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mamiksik/CommitPredictorT5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e56711e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = 'nostalgiaforinfinity'\n",
    "bug_terms = ['bug', 'fix', 'patch', 'issue', 'resolve', 'crash', 'solve', 'regression', 'fall back', 'assertion', 'coverity', 'reproducible',\n",
    "             'stack-wanted', 'steps-wanted', 'testcase', 'steps to reproduce', 'fail', 'npe', 'except', 'broken', 'differential testing',\n",
    "             'crash', 'overflow', 'problem', 'avoid', 'workaround', 'stop', 'break', 'freez', 'hang', 'error', 'leak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eadc5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bug_fixing_commits.csv\", 'w', newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Hash', 'Message', 'Parent Hashes', 'Is Merge Commit', 'Modified Files']) # column headers\n",
    "    for commit in Repository(repo_path).traverse_commits():\n",
    "        msg = commit.msg.lower().strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        if not any(word in msg for word in bug_terms): # not a bug fixing commit\n",
    "            continue\n",
    "\n",
    "        hash_ = commit.hash\n",
    "        parents = commit.parents  # list of parent hashes\n",
    "        merge = 'Y' if len(parents) > 1 else 'N'\n",
    "        files = [m.new_path or m.old_path for m in commit.modified_files]\n",
    "        writer.writerow([hash_, msg, parents, merge, files]) # writing row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdbca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sys.maxsize\n",
    "x = y//100000000000\n",
    "csv.field_size_limit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95ae88e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit: 48d71b95f22ab9141e28c711fc644cde2a69a809 | Sell with loss (stoploss)\n",
      "Commit: 1a6ca63d2fb1da07ff4b982c2baa44f9fb60ee3f | Sell with loss (stoploss) for pumped pairs\n",
      "Commit: 22bbed468cd727d6c2decae0bf7b0851f4208440 | signal_stoploss_u_b_1 lower threshold\n",
      "Commit: a0d92719c3ca0617cc1b0a51ff0b3d3a36954b1b | Lower target CMF for signal_stoploss_u_1\n",
      "Commit: c961c8be621d831313fbdd77a662ccf04bc1db5c | Fix path to hold config file. Don't load hold config file on\n",
      "Commit: 0ff33dfc7a525cfd5de20e49864b06aa43a9cfc6 | Cosmetic change to log message\n",
      "Commit: 19402692c7517146bf655743c38cb5b1c9f4de3e | populate_buy_trend conditions buy logic refactor\n",
      "\n",
      "Items and \n",
      "Commit: 5a3434acf77fcdc3b1707041958874ae7368c94b | fix: add missing buy_dump_protection_60_5\n",
      "Commit: f86f11f9328f4af8c608f025d36a9b191ab957da | refactor: resolve some IDE warning\n",
      "Commit: f1d5e67e6cf9a137b4982fb07fc16574cb32eff7 | Fix typo\n",
      "Commit: 03b30966a9332a852efc6df6c2ce78c3c922932b | Increase the max for signal_stoploss_u_b_1\n",
      "Commit: 9d8027e6b7dd16d7da8c563f4c0b7d3c58d11ca9 | signal_stoploss_u_b_1: add Elder Ray Index check\n",
      "Commit: f0e880ec786cf08e38fdc32474baec3d821cee4f | New sell signal_stoploss_u_b_2: BTC downtrend,pair negative,\n",
      "Commit: 58275417023204fa623e3e7ad4d6bc4a3e88af6f | signal_stoploss_u_b_1: test for max loss\n",
      "Commit: 47c21316bd41f39a0a0d65b0164adf1473570565 | signal_stoploss_l_r_u_1: increase recover threshold\n",
      "Commit: 516ec0313d6fb307643dfaae628e3448970c348d | signal_stoploss_u_b_1: checks for CTI & %R, plus fine tune\n",
      "Commit: c5150153e8506608fdf54b14272fc406231194a9 | signal_stoploss_u_b_2: checks for %R, plus fine tune\n",
      "Commit: ff0d792f3573983e0f903f52ef71f0dfad510a61 | Cosmetic change so log messages look similar\n",
      "Commit: eaa9b7c4d683006c18264bfc5c78326eb1fc274c | Typo and logic fix\n",
      "Commit: 47a7c8231f865c169958e0333723dd10caf4996d | Sell signal 1: break into over & under EMA200\n",
      "Commit: 5508596c3463a4ce488f71593aade2eb6b6cdcb6 | Use symlink intead of file path for hold-trades.\n",
      "\n",
      "Change Pat\n",
      "Commit: e52e347e7cd2ebfe567039c224848e102815dbb2 | Also try `resolve()`'ed path. Log that `hold-trades.json` wa\n",
      "Commit: 4b45c3462cef77ed7bf9eff767f7799eb7513dfa | Fix errors.\n",
      "Commit: 920aae11669b14d3bc44196e917be40f4964532e | Change the hardcoded value to startup_candle_count\n",
      "Commit: 3b77760626cde95ece5d8b2ac556449b5ec2e4a0 | Some minor changes to help with unit testing\n",
      "Commit: 90f5ee70ecac6feae78eeaec5dcf1c3f027cef73 | The exchange data now comes from https://github.com/iterativ\n",
      "Error on commit 90f5ee70ecac6feae78eeaec5dcf1c3f027cef73, file user_data\\data: SHA b'0be150cb0e33c29da300918c29403ce9f8d4a843' could not be resolved, git returned: b'0be150cb0e33c29da300918c29403ce9f8d4a843 missing'\n",
      "Commit: e3db643134ce3bbdbcc88927901b70c01911c1b5 | Change the returned result data format\n",
      "Commit: d42ef338fb3849b50d0860b5e69a294eec124774 | Data format changed\n",
      "Commit: 2cbc142e40d0b4fe264d95aaa2a05091bc52c8a6 | Cosmetic update for buy condtion tags. Change values to bool\n",
      "Commit: 462007aae015ebedc28b505aa455437684636c0a | style: address linter issues\n",
      "Commit: d1d23f62ec0f84a55efed15c43e33666d92bca95 | style: address linter issues\n",
      "Commit: 4561d574ceffc5404e63c597d0cce40fe6e1aef9 | Add Emulated Backtest AgeFilter logic. Update exchange downt\n",
      "Commit: 6f49688888f4a22a52e3a08f1827ac735128132d | fix: update regex\n",
      "Commit: 2117b721118a5d1b09c94b3e31a5cbd5879b5a43 | test: add test case to check for syntax errors\n",
      "Commit: 6f38ebfbb81fd2109da72051602246a2cf1e7534 | style: address linter issues\n",
      "Commit: d5472967c4ffadd2c618f76719bdf7c535969e57 | fix: replace simple references\n",
      "Commit: 4caa01cc36361dde973c5ddd24665f934e3997b6 | test: fix test case\n",
      "Commit: cb6f10c7c49e4d6a7226ab91547be044856570d2 | perf: cache property to avoid expensive calculation every ti\n",
      "Commit: c3e02dab5d4225b3bd0f2022056a5cb1330df6bf | ATR based stoplosses replace all previous stoplosses.\n",
      "Commit: d2e50a5dcaf911424c35221e45bcc3e5c2af04b7 | Revert \"Merge branch 'anasyusef-refactor/buy-params'\"\n",
      "\n",
      "This \n",
      "Commit: d4ec20ceae5a6313a47b00ff3e610bca7a4342d6 | ATR based stoplosses: fine tune.\n",
      "Commit: 56e27f43bddd11417bbfbf2d4448e73017d2afe2 | Fine tune ATR based stoploss.\n",
      "Commit: 1b48a37c7fc308981ac0a9b5924d49f82be91ac8 | Rework the ATR based stoplosses.\n",
      "Commit: 5ca2c1ef21429f3bcef4ecf74e52d5ccbe226c59 | The body of the comment is too big now, split into one comme\n",
      "Commit: 83b427c30a621c81529ecad41e735959e43f6c52 | Fix EOF\n",
      "Commit: 09b6f01c895bea67a2f1e8c0ca2a1fd174959f7d | HO params removed for buy_protection_params - update fix whi\n",
      "Commit: e1eabb98bcd3af7b4a0943bf9aef1421ffec3c56 | fix: missing .value for bt_agefilter_ok\n",
      "Commit: 99d28b591c17000a617ef79a35de515a71010b59 | Fix whitespace.\n",
      "Commit: 467d4a99ef8f3c20abda97627aecc18402f82996 | sell_quick_mode: decrease the target for ATR stoploss.\n",
      "Commit: 492af34dee5abeacfe887fd603cd81b78a340115 | Fix.\n",
      "Commit: 03f3a1c5da71e1249e2de270d8ff0fbaaa375723 | change default to False\n",
      "Commit: ca2f518039e114524a671aebd5e851a76123e833 | change not custom_info\n",
      "Commit: a43bc9d576d9be21726064f2dbbc73b874223854 | Fix `.startswith()` not expecting a `set`\n",
      "Commit: 93ccf81e95caf7fa8a497ddcd410c41f1508528e | fix test\n",
      "Commit: 718d2847ac0a13cc1cc8314867c787b036159d23 | added try/except for pandas_ta import error\n",
      "Commit: 437a2b58b4a9d32cdf5f2c2fe11606e3912401b7 | changed log type to error\n",
      "Commit: c41bed2b5bc41ecd9ef62c4e9ddfaae51a819660 | Re-add buy_tag subplot. Fix formatting\n",
      "Commit: fe8e1a143587aaf16241bd199010a3ef59986871 | Fix force_sell when no cache\n",
      "Commit: aa6f9d00a3ebd975229786e2bc1377c6246f87ab | Fix typo\n",
      "Commit: 6014e483e44e4845d1f15396cdfdedbec419e2f3 | Fix dictionary changed during iteration error\n",
      "Commit: d739598060510af02a55226620dff36dd06762f0 | fix: duplicate ci comments\n",
      "Commit: b5fe5a80c8e800067ce76e7f366f4102aa04abb7 | fix: reorganize the imports to correct the unit tests\n",
      "Commit: b151b3172a3116ffe0d3913f580d3e9ce640461f | Fix typo.\n",
      "Commit: 17f08e77ec5fb1a1f841bd9fac661ad0d643cffb | sell_stoploss: rework.\n",
      "Commit: e320038aa2e3e7633e7de439ab93fda0b27a4a19 | sell_stoploss: rework.\n",
      "Commit: 1df5cd1131758d50a903b826fbf32c483c102665 | sell_stoploss: rework.\n",
      "Commit: 44f8a713f1d629d1d6f6d6dc66add3285d68d30f | F-strings format fix\n",
      "\n",
      "F-strings format fix update\n",
      "Commit: 6942fe07f88d211a2e0eb5167841b3308dc05e84 | Fix logical bug when both ``trad_ids`` and ``trade_pairs`` a\n",
      "Commit: 697180e31a3cf7f0da9756266ceeb689297c12b9 | Another logical fix\n",
      "Commit: 82872531559ee74904e70c5a01195440ecc33559 | Set fixed stoploss to 50%.\n",
      "Commit: 5c9772fdfcabeee786e8551dd988677e1ff687e8 | Fix. Add range to test also.\n",
      "Commit: 6fc1b13e707ba076f04269a2f64e88832e2188fd | Fix. Add deviation for Binance August.\n",
      "Commit: cf4c23f94e941fb958e75da94bb00e4436c5d202 | Fix. Typo in pivot protection\n",
      "Commit: a5555bccf4db31db3e4e8a2d21f92b7c4d8457b8 | add Binance stoploss_on_exchange params\n",
      "Commit: 2c058dec011ac4b143885224432a21d7ab606bbd | add Binance stoploss_on_exchange params\n",
      "Commit: bf261128695ecfda3eec35fe32c7427f0ac749f5 | sell_stoploss_extra: Stoploss for specified buy tags only.\n",
      "Commit: b11c706ce740e9eb0d2bcc54c19fc44b7a88c593 | sell_stoploss: rename & enable per buy conditions basis.\n",
      "Commit: 72432b7a5f8ccf13101e4dfd4a08563868436962 | sell_stoploss_extra: rework.\n",
      "Commit: ebbba24c3c9e50fec5b233f24406d153a6a8f64c | Fix deubg message\n",
      "Commit: b7e5ab166c9d106a48ca65524ffdaa4b4884d97f | Allow empty buy tags to use the stoplosses.\n",
      "Commit: b64c28be17fad98cc2a7f6b23177570f6cf329e7 | sell_stoploss_extra: rework.\n",
      "Commit: 12581974aa3c7121fc9815e206ad6ece2812c214 | sell_stoploss_extra: rework.\n",
      "Commit: 80840323e94d3c89cd81e26d79b741a0e3fcb7a2 | Add all quick mode buy signals to extra stoploss.\n",
      "Commit: 7e09f2b3178edba091be09bcaf3a0c0f21411c44 | Change signal sell names to properly match with context.\n",
      "Commit: 8139b460794c3f999b1d1317c82e747d2657e3e7 | Fix for level 20\n",
      "Commit: 52c8d925728c6a43c83ec63339fadbac8aaeabdf | Fix for level 60\n",
      "Commit: 7fa969c3f957335861f98c780534ef26be963184 | Fix for level 50\n",
      "Commit: 8a1777cd7122d546bfe876f9713ad7351a9444ab | Remove ATR stoploss & rename sell_stoploss_extra to sell_sto\n",
      "Commit: 5db41f03720c2dbb8b673d070b21bdddb4b4e821 | sell_stoploss: rework.\n",
      "Commit: 39daa0ebcb2c0ed1b4ee9536dceb48d2f8c23042 | sell_stoploss: rework.\n",
      "Commit: 33e63bedf31349d65f1bcea594bd267fc1f4244c | sell_stoploss: rework.\n",
      "Commit: b6c61c42c1ebec11d6b3eab9838761b471744fc0 | sell_stoploss: rework.\n",
      "Commit: e23d196b30fa0d3072ab9c7879c762738a41e709 | Add binance deviations\n",
      "\n",
      "I want to change some buy conditions\n",
      "Commit: 6ee3c82588e85ec0d048acc1f6c47cebd128d352 | sell_stoploss: switch to ATR based stoploss.\n",
      "Commit: 7d9bbbbe9dc2746af16b204efdb8327ed16eb5f9 | sell_stoploss: rework.\n",
      "Commit: 8a3f2b532112f28ed2b2ec69cbebd901ff82167b | sell_stoploss: switch to ATR based stoploss.\n",
      "Commit: 720eec1369acfa111d45ecb0fbc3a1664e605944 | operator error\n",
      "Commit: 9693d690e9afa8a14976e20c63f8ebc835156eb7 | buy 25 change open and low condition\n",
      "Commit: 68a19c172d20eaae0ffefb821b33510992453997 | sell_stoploss: rework.\n",
      "Commit: bc4fba5f7c3c59b094934e2a4ec68979a115db48 | stoploss from ng\n",
      "Commit: f96df9b81f25c092997748e629a4291fff9b0180 | slightly modified stoploss from ng\n",
      "Commit: 93ad224ee1c33713f6bbfe246d8f31224aba2220 | ng stoploss to nfi\n",
      "Commit: ad831beb5e296945dba30a10cd63b8b4618c0e10 | sell_stoploss: rework.\n",
      "Commit: 0b2f4ef6fe148f3d2cded8f391e69074afce8dcd | sell_stoploss: rework.\n",
      "Commit: eb682f43baec7c76cb78b6bc959f77dae6fc45f7 | sell_stoploss: rework.\n",
      "Commit: 92dd480c71aa2bed1450197246abb8551636c023 | sell_stoploss: rework.\n",
      "Commit: 8a320707686e8ae6aa797c519821ec0c44b075c6 | sell_stoploss: rework. Remove unused indicators\n",
      "Commit: e2c0dce99edcd20bd6b24b941b8da6b047c41ff1 | sell_stoploss: rework.\n",
      "Commit: 5e9390ee9bd365e9a443ffb12af379af25278226 | sell_stoploss: rework.\n",
      "Commit: 0d1ef2705261635882c3db362bd7c7a2d6975dc5 | sell_pump_stoploss: stoploss for pumped pairs.\n",
      "Commit: 12db3620912f8daaf54c0f2ab713bdee454fd60c | sell_stoploss: rework.\n",
      "Commit: edca6aa99e5698532a233e2c956025bc8738ba99 | sell_stoploss: rework.\n",
      "Commit: 985e323347107d2b270606fb5276491fdc99863b | sell_pump_stoploss: rework.\n",
      "--- 10 rows processed and saved ---\n",
      "Commit: 48c0454dfd3591c96aff7957a27ea6aa70cf3b20 | sell_stoploss: rework.\n",
      "Commit: c54b090e621bc9ad7af0bd63b707a8d58d94b8f6 | sell_stoploss: rework.\n",
      "Commit: 4f6f49887768703c8ffcbb239a6abb27d1a0c6fe | sell_stoploss: add sell_stoploss_u_e_b_4.\n",
      "Commit: 929898db9f487d5a5e4ba03d140e010902807b03 | sell_stoploss: add sell_stoploss_u_e_b_5.\n",
      "Commit: 7ea0c3dc536f0dc95ba28f421398b5eb58eb3b52 | sell_stoploss: rework.\n",
      "Commit: 431a061f12c280b9ba05b1001c09de1e8140b478 | sell_stoploss: add sell_stoploss_u_e_b_6.\n",
      "Commit: 46e784a8336c8f122d7417049afddd2480266c31 | sell_pump_stoploss: rework.\n",
      "Commit: 326785c5fad80c96ba4762fb666570b3beb6acae | sell_stoploss: rework.\n",
      "Commit: 1adc10e08fa30cead595086dcec00e041966a46e | sell_stoploss: rework.\n",
      "Commit: 9540c5b36b1780b2c7825a28b8ef0c2aa8f095c0 | sell_stoploss: rework.\n",
      "--- 20 rows processed and saved ---\n",
      "Commit: 66380b1e6ba84b5ec6050daf32907d9092062e76 | sell_stoploss: rework.\n",
      "Commit: fd70e00055a75b99492ac32f2f62f3d423a4577a | sell_stoploss: rework.\n",
      "Commit: ec1ff68dc03383c792ee9733929200a3f046998c | sell_stoploss: rework.\n",
      "Commit: d9d407a31eb5e0f8e6849a57e1dbacc0d78a7758 | fix buy_tag issue\n",
      "\n",
      "Were some errors reported, reverting this\n",
      "Commit: d4b968bf40667e7d8dce6b1c4da6779d4812a511 | sell_stoploss: rework.\n",
      "Commit: 284c9639376035bbe1ed986e4e976530af932f88 | sell_stoploss: rework.\n",
      "Commit: 363a05ec26ff2a7fd7f4d2cf22b8d8a9648a0214 | sell_stoploss: rework.\n",
      "Commit: 80f3f26907fa7d30a9d2a8fee3e46055e9337d86 | sell_stoploss: rework.\n",
      "Commit: bddc80e35b8c31d4a63a42bcf75e0d51acffa4b0 | sell_stoploss: rework.\n",
      "Commit: 6bb23bdbf28e76adf619c040d0f4c2fe4a74a7ab | Fix minor typo.\n",
      "--- 30 rows processed and saved ---\n",
      "Commit: c648b7a53cdaf7941b53789695fac27d86d6b752 | sell_stoploss: rework.\n",
      "Commit: 805cff230ecee8cc3527c3f031a3626333d904c2 | sell_stoploss: rework.\n",
      "Commit: 03b900e3bea41358def2b6a6aba7d583c62ea326 | sell_pump_stoploss: stoploss for pumped pairs.\n",
      "Commit: 18338609bf1e8dd4397f0e8be15c3573b4803113 | Move tests to NFIX\n",
      "Commit: 78215a5389a2c4d173deec767876f18a57b2a88c | Also copy NFIX to strat dir.\n",
      "Commit: 54504d408e42d7d6669c49a36f8de7a5d9dade30 | Fix naming for X.\n",
      "Commit: b75f12df2adcc18c9163c9d498c889e479959841 | Fix test deviations.\n",
      "Commit: c4693155f0e21246d890b29f72f92df4067c6aa0 | Removed debug df dump\n",
      "--- 40 rows processed and saved ---\n",
      "Commit: 192ac07d118b9516c9d9a3d4aeef557b00235325 | sell_stoploss: rework.\n",
      "Commit: 572a6f15f08f3706cda6715bba6852ce4394ca9e | sell_stoploss: rework.\n",
      "Commit: 5535bfe1dc98f8540b483679069e7cc22efa9688 | sell_stoploss: rework.\n",
      "Commit: 14343abcae877f601ecd168e054e916af323e741 | manually import dataprovider\n",
      "\n",
      "needed to run the analyze_tick\n",
      "Commit: 3a0d1cd546c8d3c47a1dea4592869dba5e5ad653 | sell_stoploss: rework.\n",
      "Commit: a22da8166173bdd51da4ecd713e74f8b1868e011 | Fine tune ATR stoploss thresholds.\n",
      "Commit: 166dc99d7226b56b570c3f658572e72eb345309f | sell_stoploss: rework.\n",
      "Commit: 50ae3a8d849068a9a0a50561c157e4a69e73169b | sell_stoploss: rework.\n",
      "Commit: d5aa9927f02308c97746ebc012a51d1033123ccf | sell_stoploss: rework.\n",
      "--- 50 rows processed and saved ---\n",
      "Commit: 884a2bb467ff726f8f0542bb69bba3072b4c919a | Fix DataProvider error.\n",
      "Commit: 33cda737a06b694b63d07daee780863fb1739202 | sell_stoploss: rework.\n",
      "Commit: eba2aa8ec8a203ffefdd1e7030c5d0208dcbe9f1 | Fix signal naming.\n",
      "Commit: 5afb1f29770623c9c1dd749853fc19882d3b5f6b | sell_stoploss: rework.\n",
      "Commit: 1edb532e2639fbbe6e697250f58555f14be03518 | sell_stoploss: code clean.\n",
      "Commit: 376fbe42eeb905602d554d5a38d1da6e05626086 | sell_stoploss: rework.\n",
      "Commit: 635fa773f787548a64e0ee7de2cecc1e3719599f | sell_stoploss: rework.\n",
      "Commit: e86ec29284d59079e64b4f0681cdf87bf9d11ea9 | sell_pump_stoploss: rework.\n",
      "Commit: e16b3d1262fe68c2e238c1d133e969750ea0e487 | sell_pump_stoploss: rework.\n",
      "Commit: d66d95b51fc30ef1abf435afe1dc88f5be736b18 | sell_stoploss: rework.\n",
      "--- 60 rows processed and saved ---\n",
      "Commit: d2a41c78a6eb1c86549f7db1ab0a732df4c968e7 | sell_stoploss: rework.\n",
      "Commit: 0e7acdf4e63b7237dce99611c53d2e7a9eb1655f | sell_stoploss: rework.\n",
      "Commit: 2c506419181282c874305215338233d1ecbfc431 | sell_stoploss: rework.\n",
      "Commit: 4096ed8f1a1ad92967b888bfdf04763f3de7a499 | sell_stoploss: rework.\n",
      "Commit: aeb3ac9b62940311899a535bf1c2fbd3f356278b | sell_stoploss: rework.\n",
      "Commit: 75fb7b5fd8893212960a9f5834dd6d56e0fc6a00 | Fine tune stoploss thresholds.\n",
      "Commit: e9067bce5afdb528ed8dcc8fd2a07cfdeff06754 | Don't use the FT stoploss.\n",
      "Commit: a68103a762d2579d8feaab7667f7113c3cfffef3 | Fix docs for hold trades.\n",
      "Commit: c76b86afb2921dd62f56eaeda55d3f73a1836901 | sell_stoploss: rework.\n",
      "Commit: a503d86b1b827609ad3a03163fac6bd0aa68035c | sell_stoploss: rework.\n",
      "--- 70 rows processed and saved ---\n",
      "Commit: 8d0088eaf10c7bbe0e7b485c4b0b264413dc7f69 | sell_stoploss: rework.\n",
      "Commit: 77fe5af128adf90e713ae5acbc57094c9e6b9201 | sell_stoploss: rework.\n",
      "Commit: c4b5ef258434945621dbbb7993081af49152e037 | sell_stoploss: rework.\n",
      "Commit: 0fa7cdd921697d18a8dbeb20def7365fbc6f3d13 | sell_stoploss: rework.\n",
      "Commit: 963ca1c8a98d77fc539bcaeb29862f35cdf0fbe9 | Fine tune stoploss thresholds.\n",
      "Commit: 2936edc8b2f69163faa9c0c34df7221a72a4e46f | Don't use the FT stoploss.\n",
      "Commit: 2a16a1f65aa7d9102c0ed936d9674b38374cf7d3 | Fix docs for hold trades.\n",
      "Commit: 43da27f672bb33efc33c3767ce85f9a118287792 | sell_stoploss: rework.\n",
      "Commit: 48d587717479b241d664d93ca26c2513fdf5c9da | sell_stoploss: rework.\n",
      "Commit: 9a471740b411fc2727f46a38b950e8b2ba97576f | sell_stoploss: rework.\n",
      "--- 80 rows processed and saved ---\n",
      "Commit: 8e17d0ebd0746d891a64ae3dc225638f440125d4 | sell_stoploss: rework.\n",
      "Commit: 3d543132740e020d22a0c42cca12ea277dff4882 | sell_stoploss: rework.\n",
      "Commit: 8cc256c4ccf92891690c1216ce235944fb2c80ca | sell_stoploss: rework.\n",
      "Commit: 04d24b1bed9aa81ebef51a0e724b14799b5f14fe | sell_stoploss: rework.\n",
      "Commit: ebb9be584e22052f87704963d84e0141b0ac231e | sell_stoploss: rework.\n",
      "Commit: 1519f06a778fc1ec5f2722960b240d3f23416c59 | sell_stoploss: rework.\n",
      "Commit: 3c7dd521ccb10917fecb6eb8bcdd8d0b9de2d93c | sell_stoploss: rework.\n",
      "Commit: bd82357d6891a923557c5654a014daf606844fbd | sell_stoploss: rework.\n",
      "Commit: 71c33f0c57f6b8a43c48c2ed7c4cde42d64cd482 | sell_stoploss: rework.\n",
      "Commit: 1a1f6bdb6618502510daa1f3d28da6bf4931d380 | sell_stoploss: rework.\n",
      "--- 90 rows processed and saved ---\n",
      "Commit: ff3edd7951c54f89db3fa028aeebb9a846cb12d1 | sell_pump_stoploss: rework.\n",
      "Commit: 9453d172b153e5d502ef9fbad70d425a5a17f998 | sell_pump_stoploss: rework.\n",
      "Commit: 36e59c05ba3a1d0d06e1f0298412b828992810ff | sell_pump_stoploss: rework.\n",
      "Commit: 580a0c24fc46d41cfa823ae0e8d49f804456b7b8 | Fix DD deviation for Binance December\n",
      "Commit: 4e031aade65997ccf043c4659ec297b122e347a5 | sell_stoploss: rework.\n",
      "Commit: 606ad400c2e52ae965a13b987af28ffeb351daf9 | sell_stoploss: rework.\n",
      "Commit: bbabf0b1a410ebdaa85e94dcf87d5764689f3f6e | sell_stoploss: rework.\n",
      "Commit: 48ed8e5cb4890191c5e76a05d0174414b654b718 | sell_stoploss: rework.\n",
      "Commit: 0a43440b1a6f5ca43925865d2645a1ddcc448dd6 | sell_stoploss: rework.\n",
      "Commit: 4b55043866dcb3a04477b3343d9c503f7e5f8237 | sell_stoploss: rework.\n",
      "--- 100 rows processed and saved ---\n",
      "Commit: be18106939ce23153b424fb5c63c3b2042b87312 | sell_stoploss: rework.\n",
      "Commit: 950e2a75eafcb7371430e971b1be2ce3fc3a7077 | sell_pump_stoploss: rework.\n",
      "Commit: b8d8221396dc459acc958b12523a848ab8fb5019 | sell_pump_stoploss: rework.\n",
      "Commit: de5a27c4f1d14edbbac67d1365c749416e75afd0 | sell_pump_stoploss: rework.\n",
      "Commit: d77f8f8c876884e6784e745378d7f39f8542d7af | Fix DD deviation for Binance December\n",
      "Commit: 73b8a56f537ff2c4c0f09bb0d0653dfcb33be95b | Remove last change.\n",
      "Commit: 21692dff42296e3a9f627d84fc95da1a84e4a3c7 | Fix filename for CI backtests.\n",
      "Commit: 6dd90b5f7ecbb5c8f626dcc130503542231355ad | Fix filename for CI backtests.\n",
      "Commit: 8c31fe11babec57d484bb97192e61b248c8e9446 | Fix buy slippage message.\n",
      "Commit: c8ec5870ef357ec487d2ac11204e936259c743da | OKX links to march the recent domain name change.\n",
      "--- 110 rows processed and saved ---\n",
      "Commit: 3dbbd14ef82d479cdad1859b08978d532118a73b | Reintroduce ATR stoploss.\n",
      "Commit: ca9a32ff3b99b55c6501ca409040e1c6f4442865 | sell_stoploss: allow a short grace period before ATR stoplos\n",
      "Commit: 13e4064b3d3ae0e28330653ed6030749b1fb7fe6 | sell_stoploss: rework.\n",
      "Commit: c719c3263d35d9b340c455b1f7fd58f12eb57497 | sell_stoploss: increase the trade duration threshold check.\n",
      "Commit: ac08c95c506ba0a6aac32333d3464d57cac5a668 | sell_stoploss: remove ATR. Introduce new.\n",
      "Commit: e62ec17c21dc3440776bdb8c42130bf8d821621c | sell_stoploss: rework.\n",
      "Commit: 7ee54050650d572f2f89d3676565541be4e0704e | sell_stoploss: rework.\n",
      "Commit: 42c19756d6bb77483bca3b8b213a4ebfc9e49741 | sell_stoploss: rework.\n",
      "Commit: 8d4cad25006c5d5cdc8b20f2cb90eec8a3736966 | sell_stoploss: rework.\n",
      "--- 120 rows processed and saved ---\n",
      "Commit: d77ac8f17dfe410a51388fe31836ee0d6fa85be0 | sell_stoploss: switch to ATR.\n",
      "Commit: 613fc961eb56cda5ab8a0dfa48ee47dea04bd678 | Stoploss: fine tune the ATR thresholds.\n",
      "Commit: b3e3d4124daca030383d32de64dad6844d825ddd | sell_stoploss: rework.\n",
      "Commit: a3df837096a4dd1dcf8ee2515cb895fe80930f2e | sell_stoploss: rework.\n",
      "Commit: 83c0d710370d4765eca610e5745565b1c74b9407 | sell_stoploss: rework.\n",
      "Commit: d4012c429e976510427239073694c89decd254b5 | sell_stoploss: rework.\n",
      "Commit: 3db7a581cda2253feff9e3d459aef3f75e4f7550 | Allow changing of the initial stake if rebuy is enabled and \n",
      "Commit: 8a2031cde3e648ea19ccbce78e3155e42826010c | Stoploss: fine tune the ATR thresholds.\n",
      "Commit: 7c2cb26fbb6de3e742372afd8d5d60527c16f6b1 | Fix typo.\n",
      "Commit: c42f37e2e2d91906e6f45fa36a65a5dcdd0706ca | sell_stoploss: rework.\n",
      "--- 130 rows processed and saved ---\n",
      "Commit: 6ef66c7b97251b00798b0caf7be914e0ed96e83f | sell_stoploss: rework.\n",
      "Commit: 9adc1a8c036badf4533edbf31a08274f4a181a04 | sell_stoploss: rework.\n",
      "Commit: 09f4e0a813bd464dea15cb23593f77d3f81259b3 | Remove sneaky debug messages\n",
      "Commit: d18d6b1d7f7b8bdf3be522ef7cfcf2778e8048cc | X2 fix EOF\n",
      "Commit: aeb0121e6444073beaee5c13f6fc1eaef5378ab5 | sell_stoploss: rework.\n",
      "Commit: 3bc8fdd584fbe501bc7111d3eaa2f95cf2b65b3f | sell_stoploss: rework.\n",
      "Commit: 73960c1866a60a12b5e618eb0a2b5783b2778253 | sell_stoploss: rework.\n",
      "Commit: 6a4c743b83a3e7e4f527d74e20c78579e7657667 | sell_stoploss: rework.\n",
      "Commit: f46a84c80293d71c0646a7163cfe1cf0a6bf3da2 | sell_stoploss: rework.\n",
      "Commit: fc2fc7a57db959390ad2f994e56632b8d06e0af2 | sell_stoploss: rework.\n",
      "--- 140 rows processed and saved ---\n",
      "Commit: 5acac33534d41892682c72da84161b0b8e4668a5 | sell_stoploss: rework.\n",
      "Commit: d8dc8a2b7ba589109185335a0855f757c9f2946f | sell_stoploss: rework.\n",
      "Commit: dd2ea833c32be5c1b4df843d1ad812b33c888f59 | Stoploss: fine tune the ATR thresholds.\n",
      "Commit: c965cf31c258245e41252926da2dc80e84dc8c8c | Stoploss: fine tune the ATR thresholds.\n",
      "Commit: b1827945d2e7d26eb1839959ebc34dcbf3dae011 | sell_stoploss: rework.\n",
      "Commit: 4373c0d7eb920ad841b4e80af46dac49a04af246 | sell_stoploss: rework.\n",
      "Commit: f5538c67ed3b39e182e0965715d7ddf8e85dec4b | sell_stoploss: rework.\n",
      "Commit: ecdf53c6020991012debb2d81af2bf4ad8e91e93 | sell_stoploss: rework.\n",
      "Commit: 91fd77ecd6041f85fbc424fed7a99949e20da10d | Fix DD deviation for Kucoin January 2022 bt\n",
      "Commit: 38842e78a78e4d31128fc0370c122179467a0fa1 | sell_stoploss: rework.\n",
      "--- 150 rows processed and saved ---\n",
      "Commit: 6b88554db99641e162270d472312f12436fbcb6b | sell_stoploss: rework.\n",
      "Commit: bbe0942f17c052cde234b3817a1df23348363502 | sell_stoploss: rework.\n",
      "Commit: 8945ae4432b70ab479f0739125c6a455743b97e0 | sell_stoploss: rework.\n",
      "Commit: 291c449a66ce661fdf82da21696d679e294ca459 | sell_stoploss: rework.\n",
      "Commit: eeb0bb6be1483c1a7fb813df1ff0fc4319baa960 | sell_stoploss: rework.\n",
      "Commit: 47e5ec541b750d3b129a3484ba4d143c52cbad5c | sell_stoploss: rework.\n",
      "Commit: 873e2928f199975ca011a003199da768ade218ac | sell_stoploss: rework.\n",
      "Commit: af2ea70da72cf4dcc3b15425c421a70c315cf674 | sell_stoploss: rework.\n",
      "Commit: 67995bf032632c743d18cb9932c605af19dd8319 | sell_stoploss: rework.\n",
      "Commit: b9d4a72e19a8aa94721df9ef2ead2bdd80881d44 | sell_stoploss: rework.\n",
      "--- 160 rows processed and saved ---\n",
      "Commit: 7214035e1520ad8e19d70435e1e15baf964668da | sell_stoploss: rework.\n",
      "Commit: b0dd54a753d593292bea187f4d85d258b4510395 | sell_stoploss: rework.\n",
      "Commit: df76a51f28bbabae0334d1e9fcce0f62585e508a | sell_stoploss: rework.\n",
      "Commit: 96d1277efcb2c7f91dfbf3c288d34932c3f2f08e | sell_stoploss: add stoploss doom.\n",
      "Commit: 75037707ff5c10731a99aec2bf197cae2089287d | sell_stoploss: rework.\n",
      "Commit: 290781be05c9d8631fe828d5f7ed77bdebe1c821 | added separate stop loss for leverage tokens\n",
      "Commit: f1fa2086b241a603f1f0675ef5e3fe701ef9d9ac | added separate stop loss for leverage tokens\n",
      "Commit: 64f3db6ec41a586cf02a55efab9a1e4b2b422392 | sell_stoploss: rework.\n",
      "Commit: fe5d5d802623c73e6315cee3d09bed03691f7742 | sell_stoploss: rework.\n",
      "Commit: 9ef443d407311a4f855607955c9ea650d83579d6 | sell_stoploss: rework.\n",
      "--- 170 rows processed and saved ---\n",
      "Commit: c1967caf4979431ba30895ec299aed9547efd568 | sell_stoploss: rework.\n",
      "Commit: 63d0baad125b8b364233713c14d03781f2411801 | sell_stoploss: rework.\n",
      "Commit: e441ed897dfb6d81a7be1f72c678ea0a53ef2ba1 | sell_stoploss: rework.\n",
      "Commit: 58d91735d3d5a1ecc7bd97835b39122ae0b16ca9 | sell_stoploss: rework.\n",
      "Commit: facf4f0ed0e60e814c8b0329798420b120c10d45 | sell_stoploss: rework.\n",
      "Commit: d2142199cacb356d92b5f7342ab7b7b3ba08af00 | sell_stoploss: rework.\n",
      "Commit: f8eee81e115bf8b7ef1919c87ad5a563d94924b1 | sell_stoploss: rework.\n",
      "Commit: 9d56978f027c1a08ca4eeafb4cf3c5fe0334e5d2 | sell_stoploss: rework.\n",
      "Commit: e1be97868373c213801ac62aac8aefb4046ebb57 | sell_stoploss: rework.\n",
      "Commit: 650001ccaa16ccbc509c5a3aa9dee4041cce1eb7 | sell_stoploss: rework.\n",
      "--- 180 rows processed and saved ---\n",
      "Commit: cc28858874a44391968eddd4ff4fe2f865e34ec7 | sell_stoploss: rework.\n",
      "Commit: 3b60d978fde42e10e871d4594ee326d25d861249 | X2: add sell_long_bull_stoploss.\n",
      "Commit: 6f869d0b82561470efba08854d915ec0762dc290 | Fix backtest ranges.\n",
      "Commit: 39406e2978b0813ea7e3848e42242779093b7b7e | Fix S/R functions.\n",
      "Commit: 9733d95ed8d80c43a3595309c099fef2df79d7e6 | Fix S/R function\n",
      "Commit: 6cdcaf0758b2d81d031253337624cc3c5120d152 | Fix compile issue due to S/R change\n",
      "Commit: 0fc2a7183a4f4652c6349601e1922b156f3a13b3 | Skip trailing_stop_loss too\n",
      "\n",
      "if initial stoploss value not m\n",
      "Commit: 71cc12c93cc68295c89d9bc6c371b2c9c1e74ec0 | Fix parenthesis.\n",
      "Commit: dc998f88417930ef7e55744f08b6a25136b459f6 | confirm_trade_exit: allow trailing_stop_loss.\n",
      "--- 190 rows processed and saved ---\n",
      "Commit: b8285248183e888e8e9841ccfe6d4953e1a9b148 | sell_stoploss: rework.\n",
      "Commit: 224f9b0add9184985f6add54112b1e7b3298992b | sell_stoploss: rework.\n",
      "Commit: c08ee31d3eaddb8675d21a469933c5333fd3cef9 | sell_stoploss: rework.\n",
      "Commit: 8b769002b43685a26ac37783a0afae0c192a23f0 | sell_stoploss: rework.\n",
      "Commit: 5d559d6caa28eae435a21b8757ecf39a6253426b | sell_stoploss: rework.\n",
      "Commit: 0e5b6815e7d7cbeda1b7cbb455154a7685810aa0 | sell_stoploss: rework.\n",
      "Commit: 10fb06ee50b450bc6007f4b3917509d70fb0b00a | Fix variable name.\n",
      "Commit: 3789ee428d90c0187efe2334f27a95b6180348fb | Unit tests: change SellType to ExitType.\n",
      "Commit: a21a342afa9908d456229a069165b039ce6561f0 | sell_stoploss: rework.\n",
      "Commit: 7a4e8712b6467c44dcf5dc37a4149cb36f1034df | sell_stoploss: rework.\n",
      "--- 200 rows processed and saved ---\n",
      "Commit: e7910efc619ca1aba6d0adff494d7ee26e339ae8 | sell_stoploss: rework.\n",
      "Commit: f7db1eb135151ff93ec506c65abe22e3504395f7 | sell_stoploss: rework.\n",
      "Commit: 6f11e468d7af7054410975f27759c05e3ab252a0 | Fix typo.\n",
      "Commit: 98ae531a839eb3e2e739160f0701c22717dcd1a8 | sell_stoploss: rework.\n",
      "Commit: 84e6a38403bf96da4bde2eb2177281de6d444e8e | sell_stoploss: rework.\n",
      "Commit: 56d26c10f161be89fd25e10b0dee4db106d42e58 | sell_stoploss: rework.\n",
      "Commit: 78936854ca14d75c7b7a59391c4cc5bf358bd056 | sell_stoploss: rework.\n",
      "Commit: 4bc8abcebbd9cde36b9868a58e8874cbacaf07fc | Fix mean volume 4 indicator.\n",
      "Commit: 2b57f57904e2ec44eb166a0071d7bf1551173c76 | sell_stoploss: rework.\n",
      "Commit: 9f215eff31eed34e3888d4832b8dd3dccd440c99 | sell_stoploss: rework.\n",
      "--- 210 rows processed and saved ---\n",
      "Commit: 3780cff5cfce954cbbd3e9dee1f443940dc440b5 | sell_stoploss: rework.\n",
      "Commit: 6fe278420acced5edd75e22102d4b8f4597f282e | sell_stoploss: rework.\n",
      "Commit: 2ed94278a6a7c11857ab8fda01065e6ee6355111 | sell_stoploss: rework.\n",
      "Commit: 0ad05f87d9ba647057e2193824fb49c64316def9 | sell_stoploss: rework.\n",
      "Commit: 6877bd179ae274969d998acdd2770d0b9195881f | sell_stoploss: rework.\n",
      "Commit: e70d12e16250f4080285a881903fdb015d9dc0d9 | sell_stoploss: add doom 3, safety net.\n",
      "Commit: 9aa1d12065ec5636a758a63ab5915198528f567f | sell_stoploss: rework.\n",
      "Commit: 400499a2eadca6eb8b2b2de482cb20d3a32d315a | sell_stoploss: rework.\n",
      "Commit: 3c0a7bf58322ba4133bb32da3a4f0e272aaf665a | sell_stoploss: rework.\n",
      "Commit: d4bc2457d73ff162b7352c5e489a6f59a12b3886 | sell_stoploss: remove doom 1; 2 -> 1.\n",
      "--- 220 rows processed and saved ---\n",
      "Commit: 2ef700dc37b6872c40113362df518cb4fd40007e | sell_stoploss: rework.\n",
      "Commit: d36c5c955df21e923b729a9d0c9bee89d2eab2f8 | sell_stoploss: rework.\n",
      "Commit: ff6677d6c6a77aea48d2faa8b9e9dfaf748ac38b | sell_stoploss: rework.\n",
      "Commit: b3cbc6d5dd52be57b80c18d39c7ae02edbf86e15 | sell_stoploss: rework.\n",
      "Commit: 3cac1f8637e2cc27b8ca15dec350a87e24d0ee25 | sell_stoploss: rework.\n",
      "Commit: 8eb4f7823b6510d7db680bd5d22aeec0fe8bb26e | sell_stoploss: rework.\n",
      "Commit: 6fc1811ca75a4cd947e6ad1544c3397041aeefdc | sell_stoploss: rework.\n",
      "Commit: f3703828898a71a17c8f6de5e49ed03458a5ae8e | sell_stoploss: rework.\n",
      "Commit: cbc431f0e9467fef971e4d7afa2171ca223cbe0c | sell_stoploss: add sell_stoploss_doom.\n",
      "\n",
      "For situations with \n",
      "Commit: 52b81aa65f4018989ef0b11ec109c4ec1f2e8193 | sell_stoploss: rework.\n",
      "--- 230 rows processed and saved ---\n",
      "Commit: ec5e00fab150b2caaf2c0fd4bed08787c714cbb9 | sell_stoploss: rework.\n",
      "Commit: 1f663d8bdc0143af01ea73577131c1cfe60ef852 | sell_stoploss: rework.\n",
      "Commit: a5b0dc6fd5458aa45f172b2114cd8b06adc861dd | sell_stoploss: rework.\n",
      "Commit: 67b23d7f4fc0b948ab149870f97092c0e7561258 | sell_stoploss: rework.\n",
      "Commit: b41dc821b8484e51352911d69f082c7662d84773 | sell_stoploss: rework.\n",
      "Commit: cdad3a3c968bfd817e41a461f644bbaebdb830db | sell_stoploss: rework.\n",
      "Commit: 7756638a956693eef6e2cdcdc9775d44bbd2c859 | Profit maximizer: turn on for the stoplosses.\n",
      "Commit: 56011f23b589dd165db30afbc02c8074113a70ac | Profit maximizer: trailing 0.5% for the stoplosses.\n",
      "Commit: cdd45cf8674e3a83b1fccd4e2ba77b4017efef59 | Profit maximizer: unique filename per exchange and stake cur\n",
      "Commit: bc064a25f914a8ff96e11eb21ffef77eaf43cb4a | sell_stoploss: code cleaning.\n",
      "--- 240 rows processed and saved ---\n",
      "Commit: e80e6f24ef7e6d028db6843414d6b6f778538896 | sell_stoploss: add an absolute lower hard limit.\n",
      "Commit: db47cbf7914b12cba0cdbfe196f15c0fe256a36e | sell_stoploss: rework.\n",
      "Commit: 2a5f9778a9b87bda94bab5d244bfcd43cb764518 | sell_stoploss: rework.\n",
      "Commit: 9678dbe44bbd44990fafc2fdc8636ee2df5971ff | sell_stoploss: rework.\n",
      "Commit: 5488e0f61d78ae894f9dcbfffd929c9fd6c5fa21 | sell_stoploss: rework.\n",
      "Commit: 0980aacb16352997a05d0e4b2dc1d93bf3259c8d | sell_stoploss: rework.\n",
      "Commit: 1d9cff272089d7fb77bc9e9ca31991e6684b4482 | sell_stoploss: rework.\n",
      "Commit: c9b81fff9eec8d6ff4fc8f918b65bec3ca767a9c | sell_stoploss: rework.\n",
      "Commit: 6ea5eeb930fccc508542ec96091ee754b5945579 | sell_stoploss: rework.\n",
      "Commit: 699a79c0978d6fa7f22a65a509fa4864f6529129 | sell_stoploss: rework.\n",
      "--- 250 rows processed and saved ---\n",
      "Commit: d223e828e01b41a6d94286f3af048911ba1e1002 | sell_stoploss: rework.\n",
      "Commit: 3cd693d64663465e32c4ce38d1e4870a526b1824 | sell_stoploss: rework.\n",
      "Commit: 1803e73cff8f017ee30703843b00f1cd0b928c28 | sell_stoploss: rework.\n",
      "Commit: c74db6ce6e4f00797afd7342482d727a9a514c13 | sell_stoploss: rework.\n",
      "Commit: ca9e509834883a8059fd4c5351bdd9c28ac65e2d | sell_stoploss: rework.\n",
      "Commit: 34bde5bed8b6e2a504967da2d06b65ba1b9a12b2 | sell_stoploss: rework.\n",
      "Commit: 56a3c2be42d9863b90a1cf0f5ce0c2482ccd0361 | sell_stoploss: rework.\n",
      "Commit: fe48c2affff053fe74c4ebef2502a158736e0468 | sell_stoploss: rework.\n",
      "Commit: 9850748d4eaadd903709d0d5d5fa13409661b8b7 | sell_stoploss: rework.\n",
      "Commit: ecf6b2f44905e9d19c504272efa1ed6023584f14 | sell_stoploss: rework.\n",
      "--- 260 rows processed and saved ---\n",
      "Commit: 8c5547c9b263cf0f64bf8d519fd9532470591ed4 | Maximizer filename: fix if bot_name not set.\n",
      "Commit: 7ecb3c5ca5b2b0c8a627bd9288569f0c5f7ae5ff | sell_stoploss: rework.\n",
      "Commit: 8e8f1f448e5bcd8aacc8f8402ec9c0d94dd772b6 | sell_stoploss: rework.\n",
      "Commit: cfae9a913a9eef9e92a7976218ad2954e37a0bad | sell_pump_stoploss: remove.\n",
      "Commit: b90b46a26ea01f3b1fd490275df0bffb88d4d9bf | Rebuys: mode 2: fix for low stakes.\n",
      "Commit: 6e65b0c84c7dfb3e3863e7923fb80d7ee597af58 | sell_stoploss: rework.\n",
      "Commit: 399384c818ec4f75e4a5e218466b1c087ec7e6c5 | Maximizer: allow sell_stoploss_stop_2 to use maximizer.\n",
      "Commit: 91d076a4abfa9b3822d9cbb7904d3dbdb7e268a2 | sell_stoploss: rework.\n",
      "Commit: cb2105a59c6b850334817a9c2b6586a3f657d9ce | sell_stoploss: rework.\n",
      "Commit: 03305ba6374110457268db5ade1caaddc6f19f3c | sell_stoploss: rework.\n",
      "--- 270 rows processed and saved ---\n",
      "Commit: edb3246cb582b19ed5d93341283491805583458e | sell_stoploss: rework.\n",
      "Commit: 73fa1267580c0540fa9cd0975384070871054e08 | sell_stoploss: rework.\n",
      "Commit: 662b5a20fc4348fbfc0984e6bd2046fbea3024e3 | sell_stoploss: rework.\n",
      "Commit: f54c7a4a6a7d5f814ed5ed7195d6e705f2ca9483 | sell_stoploss: rework.\n",
      "Commit: 28687d23eb28e655006546f978a1dded5656c8f0 | sell_stoploss: rework.\n",
      "Commit: 46127c43052240b07281841a50a2c014b0f77ad0 | sell_stoploss: rework.\n",
      "Commit: 0c3c2a93828f4446ecebeefb33c08f3ad9fa2446 | sell_stoploss: rework.\n",
      "Commit: b174bc9e04fbf527ff3be77503ce6a711da32897 | sell_stoploss: rework.\n",
      "Commit: 76d48bb6417796c9f4e7353bff8a8771cda58c15 | sell_stoploss: rework.\n",
      "Commit: 58e46ad2fdc7e6458d4906c15a8fdaed3bd01927 | sell_stoploss: rework.\n",
      "--- 280 rows processed and saved ---\n",
      "Commit: 53007bc0c96415e3ff55938db609041fd62fbbef | sell_stoploss: rework.\n",
      "Commit: 1ef2a2ada89c72f545d5a58fea6a9559298cc3c2 | sell_stoploss: rework.\n",
      "Commit: 7a62a8b1accb80a6eecc77a84ede05571d090066 | sell_stoploss: rework.\n",
      "Commit: ed19a0d2051fecc3cb568fe93d018c64cf78934f | sell_stoploss: rework.\n",
      "Commit: d4c24a062bbd315aecc41599479348ec14b8baaa | sell_profit_target: if sell_stoploss_stop_2 reach over a thr\n",
      "Commit: 1300973b2e64b7d99273704cfea1e4b65540d32d | sell_stoploss: BTC stake to -16%.\n",
      "Commit: e9f5078c1d4b36ac75c34417520b5c525542a95b | sell_stoploss: normal stake to -16%.\n",
      "Commit: f26be7aad61035cc1c9b29b73ad9829a87a3bb96 | sell_stoploss: normal stake to -18%.\n",
      "Commit: 003cf6bafc80df7e5b46c4ff0abe1ab3e22045a2 | sell_stoploss: normal stake to -16%.\n",
      "Commit: 67ad90c550ce1c7c7a2dcdba36e72b48c440e0df | sell_stoploss: BTC stake to -18%.\n",
      "--- 290 rows processed and saved ---\n",
      "Commit: a73d262f660aba088850faecfd2ce3fe946f90d3 | sell_stoploss: rework.\n",
      "Commit: bed0147eb0a7ad9b158b19e86cfe6d87f8dd1d8d | Add ema_200_pct_change_144 indicator.\n",
      "Commit: 06a111902cee868f2d38611f64e0ada009b053aa | Add ema_200_pct_change_288 indicator.\n",
      "Commit: eb0d172d1ad4abfc3aac9d372295066fb613866b | X2: exit_normal_bull_stoploss: rework.\n",
      "Commit: 7979f98b73ed60f7cd4e0cfcff093195b4a7b3f9 | sell_stoploss: rework.\n",
      "Commit: 4a11753f595a844ec3737bd8e344590e849649de | Fix plotting issue.\n",
      "Commit: 97fdf06701aec788a6c96874231a80105ee9f607 | sell_stoploss: rework.\n",
      "Commit: fa99ed7567453f3a00a6cf4209ca17d409125db2 | sell_stoploss: rework.\n",
      "Commit: f5aa65494d4658b28ff4f97d9b9806e2c65702b4 | sell_stoploss: rework.\n",
      "Commit: 59284c818f0f1bf4a3c1e00f0e6e4e4f9c33c003 | sell_stoploss: rework.\n",
      "--- 300 rows processed and saved ---\n",
      "Commit: ea6954a49a25cbecc7b6c90ed0c26710f2987d11 | Maximizer: remove stops from target update.\n",
      "Commit: d1b8b9016313be64b20c3e257a41f52f3f468c8f | sell_stoploss: rework.\n",
      "Commit: 4079de7184ce1828d9122ea7e6ede6d6aad5a359 | Add configurable stop thresholds.\n",
      "Commit: 243bcd4d1d6f6bcb7d85fb2f07188d1c270fa19b | Fine tune the stop thresholds.\n",
      "Commit: d34cb16c39365179f596461f99fba970d30ab578 | X2: exit_normal_bull_stoploss: rework.\n",
      "Commit: b95d1344a0e152e65e64747af04f7c69de8e8bb7 | X2: exit_normal_bull_stoploss: rework.\n",
      "Commit: 06ade8fcd22a8c759041c04e67e0408bd5cd62bf | X2: exit_normal_bear_stoploss: rework.\n",
      "Commit: 2270f567923631b49471d9739458955b5f78f2c6 | X2: exit_normal_bear_stoploss: rework.\n",
      "Commit: c98d26a6312969397cf58a897747ea9d863f29b0 | X2: exit_normal_bull_stoploss: rework.\n",
      "Commit: 2e8b9b43bff0706e811e8ad23379a72fad6a4fa4 | X2: exit_normal_bull_stoploss: rework.\n",
      "--- 310 rows processed and saved ---\n",
      "Commit: 455fa79050ae21ab9380e2c4bf1dafde3c53d9ed | X2: exit_normal_bear_stoploss: rework.\n",
      "Commit: df1f93953ec61ba4101fbdddbd4bf6ec6aedeb66 | X2: add configurable stop thresholds.\n",
      "Commit: 2da51ec1206c13ba0d9277ba84d8b125dbfdacbe | Changing the comparison of checking a singleton value\n",
      "Commit: 12551f483f110d95cec03287537c437c781d78f6 | Changing calculations in the adjust_trade_position function \n",
      "Commit: 5c3579dc7898ea7781b03eacfacded4e2a3557be | Fine tune the stop thresholds.\n",
      "Commit: 076f0ef4d6283b0fb9ad58aab1f83deaa0ac092d | Canceling changes to the comparison of singleton values, mor\n",
      "Commit: a612dc1aa2586aaf321478858625f5873c094991 | X2: fine tune the stop thresholds.\n",
      "Commit: 027210b9dcbacadfde92847c87317f6c26239469 | X2: fine tune the stop thresholds.\n",
      "Commit: 8b4fdb348de2d594e1aee2986ede8d4de27f6c8e | X2: stops rework.\n",
      "Commit: 15a023ad7eb5dbb751755471b9f2a4b25620616f | X2: stops rework.\n",
      "--- 320 rows processed and saved ---\n",
      "Commit: b53906c5ad16f6c994ab65317342033b5e236b5c | X2: stops rework.\n",
      "Commit: a558904d6085e951eee941a91af562110576a268 | X2: stops rework.\n",
      "Commit: 3a491bd3f9a4ea246f2ccef2a93b9adadf863819 | X2: exit_normal_bull_stoploss: rework.\n",
      "Commit: 562f4084ed5af6a8a2713f14b2f0c78ca43dbe3e | X2: exit_normal_bull_stoploss: rework.\n",
      "Commit: c1e081fd793d0cb5f6554f3ac71cead2e2d98cf6 | X2: exit_normal_bear_stoploss: rework.\n",
      "Commit: 888ec81979585261d0ed3c5d74ed7b9fa4cad677 | X2: fine tune the normal stops.\n",
      "Commit: d1071187ae2f6c57fe9dd4dc1ed00ac7fb0e3515 | X2: exit_quick_bull_stoploss: rework.\n",
      "Commit: 49129f7e0acfd6346e597ed482dc74e3337c2a2b | X2: exit_quick_bear_stoploss: rework.\n",
      "Commit: 33b5da32cc1d79cd7dd684b085895e879e2303b9 | X2: fine tune the quick mode stops.\n",
      "Commit: e87f9330ad5bf2f178cc40bff4ede09c13124a66 | X2: exit_pump_bull_stoploss: rework.\n",
      "--- 330 rows processed and saved ---\n",
      "Commit: f063da6ea5460d369e1c9e312791ac4a56a0a529 | X2: fine tune the pump mode stops.\n",
      "Commit: e0fa07254703113c8820ba413130a6aa49f5b1af | X2: exit_normal_bear_stoploss: rework.\n",
      "Commit: ad8ab814b1a7580cb46d3f345567005ed8417b7f | Fine tune the stop thresholds.\n",
      "Commit: 5443573aa58e1db36ee39d9fd58d9bca012d197c | X2: add stop thresholds for rebuy mode bull and bear.\n",
      "Commit: 51118631961146d8363a8a1e31d96c3e18ea9ccd | X2: add config options for rebuy mode stop settings.\n",
      "Commit: 88d9cb0303396b9f46e21dc2d49b0adab374d9f9 | X2: add long mode stop settings.\n",
      "Commit: 73da8e9937f3ed1d0f7c92791b7ae0e2f93ecfcb | X2: fine tune the stop settings.\n",
      "Commit: ecedd99a8f4561d0c96fa330a956690100460846 | X2: fine tune the stop settings.\n",
      "Commit: f08791ffa63e0fc3fc29affe84e7b98e761eeff6 | fix: enter_tages UnboundLocalError\n",
      "Commit: 19adca67ee943b0014b1e14d97e7aeb1fd54649e | X2: fine tune stop settings.\n",
      "--- 340 rows processed and saved ---\n",
      "Commit: 6953946e9d0ab790d86a857f3424bc361cebc59e | X2: add exception for OKX startup candle data number.\n",
      "Commit: 55a2955f0e4cb1d7449839c74545ac4b3d4274b2 | X2: add exception for Kraken startup candle data number.\n",
      "Commit: f95a171f1ae565a935e436289f47d82d85fd1c45 | X2: fix ident.\n",
      "Commit: 46101fcdb649f5e0ecec07e87f88400f6f38bf03 | X2: grinding sells: change order amount -> filled.\n",
      "Commit: c7ffac45cdd5cb3b69bf463a90206c14431950a6 | X2: fix hold functionality if there are partial exits.\n",
      "Commit: e386aea8987541974444533ef9a48f085f30b575 | X2: fix if there is profit only set to true and partial exit\n",
      "Commit: 4f770ed680f3091892d2f7237005cf4051948e24 | X2: grinding: fix sell check.\n",
      "Commit: 409264653d8e2845fa3b7e11c7068f474f616baa | X2: grinding: fix sell check.\n",
      "Commit: a2c6d549a64590a98ecb295f7a6e009152cbfd8a | X2: grinding: fix sell check.\n",
      "Commit: 6db3990571cd876d6c6bb3adbc33a6efd427c446 | X2: grinding: fix msg string typo.\n",
      "--- 350 rows processed and saved ---\n",
      "Commit: 003fad0b33b94a71ab3f4cbc0472d2255e8ce4fd | X2: grinding: if low stake, change only for the current trad\n",
      "Commit: 03042c467970d197ff363055d8809494fc4a13b1 | X2: stops: use doom, relative to the trade open rate.\n",
      "Commit: 0512e9178abd50bee543e019d23130dfc8238a46 | X2: grinding: fix sell remaining grind if partial fill on se\n",
      "Commit: ec517883abc4f8ce308280895bace236107c7abd | X2: stops: use u_e, relative to the trade open rate.\n",
      "Commit: d57367e8dab6414712bd7c056dfc49f7e8c901af | X2: grinding: fix BTC checks.\n",
      "Commit: a4f7ccd7efa98f39091813ff76f0cd881f04caa0 | X2: rebuy_adjust_trade_position: fix BTC check.\n",
      "Commit: efd2fad5b423c411896684ea9d2564fb61932d34 | X2: change startup candles to 199 for bybit.\n",
      "Commit: bb5c6004885681cfc6963d4bd30ed46b2a48c63a | X2: disable the FT stoploss (X2 use different stops).\n",
      "Commit: 9016b2dfd004dcd33e0ad46149a15b2848b3d8d7 | X2: grinding: fix for partial fix on exits.\n",
      "Commit: 18acc0be9f968e2419174d25237980a732c1e529 | X2: maximizer: fine tune thet railing for stops.\n",
      "--- 360 rows processed and saved ---\n",
      "Commit: bef24aa15c72fd2955f9fc540b97fb0f78a93293 | X3: fix bot_loop_start() call.\n",
      "Commit: 99c0d78d5fb3c153a29112ffed4aeff4e5ce0933 | X3: fine tune stops.\n",
      "Commit: d6fefe2bfd3091fc14126fb128a1ad3a024287cf | X3: stop is config option now.\n",
      "Commit: ef49f5764ef9edf37137b1490679f12dbd4c3489 | X3: enable stops for init and grinds. Use full slot stake.\n",
      "Commit: cf527700feb01cdfe3b228514868f58fab29074f | X3: grinding: stop init buy, check for min stake.\n",
      "Commit: 145d6a4ca560077aff4f85262d741dfc49fb46d3 | X3: grinding: sell 1% less on stop grinds.\n",
      "Commit: 1ac646525d4c6e09f2554169d8b6e7e1fcf55790 | X3: fine tune stops.\n",
      "Commit: 8124655311c11f93c55368585f68cb3fdffe4c21 | X3: grinding: sell 0.1% instead of 1% less on stop grinds.\n",
      "Commit: 8a3194ba4a76a885989caf03906e76c8151bfa36 | X3: grinding: add variables for stop thresholds.\n",
      "Commit: 284edf80c62a2d1833f0ce256472db693085289d | X3: grinding: grind only after stop init.\n",
      "--- 370 rows processed and saved ---\n",
      "Commit: a98a851ddd24598f38fee6080b5fe1e104d33e0e | X3: grinding: stops to -8%.\n",
      "Commit: 76365dade594791d481307632ca9f2b5c3a95733 | X3: grinding: use current stake profit for stop init.\n",
      "Commit: cb78030043217cc271f50f12a599d1d6dcf4863f | X3: grinding: allow grind stops for old trades but new grind\n",
      "Commit: 7fa863a9a4cd98b0fd1edcffb83747581b0b0865 | X3: fix for earlier than Python 3.9 compatibility.\n",
      "Commit: a42b72a8e8a04f90c36d2417f0e3fc11e5c5cd43 | X3: fine tune stops.\n",
      "Commit: b4227577cfc006b12d7453ede61a7eebd8da4541 | X3: grinding: stop & profit thresholds config options.\n",
      "Commit: 7fb98992755a37081586ee88c26ced3163fc6bc7 | X3: grinding: fine tune the stops.\n",
      "Commit: 40963898d8a4e9277511ff0ca05d4bc805859134 | X3: grinding: fine tune the grind stops.\n",
      "Commit: 904f28c7de05cef2503cab25bf6dcff6d54af024 | X3: grinding: sell the remaining after grind init stop parti\n",
      "Commit: a9249dbf1172140a850b1cf2921c5e821b4b4d31 | X3: grinding: sell remaining on partial init stop: check if \n",
      "--- 380 rows processed and saved ---\n",
      "Commit: 10a168d19c71445c77b86a8f272e52690c3210c1 | X3: grinding: fine tune the stop thresholds.\n",
      "Commit: 355bed6f5418a0c339cb0717b6a9f333f1d5b5b4 | Some fixups and improvements\n",
      "\n",
      "new file:   .github/workflows/\n",
      "--- 390 rows processed and saved ---\n",
      "--- 400 rows processed and saved ---\n",
      "Reached 500 file changes, stopping.\n",
      "Finished or interrupted. Progress saved.\n",
      "480 commits processed.\n",
      "A total of 400 file change instances processed.\n",
      "Results written to bug_commits_rectified.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = \"bug_commits_rectified.csv\"\n",
    "\n",
    "processed_rows = set() # to skip already processed\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', newline='', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header\n",
    "        for row in reader:\n",
    "            if len(row) >= 3:\n",
    "                processed_rows.add((row[0], row[2]))  # (commit hash, file_path)\n",
    "\n",
    "with open(output_file, 'a', newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # Write header only if file was empty\n",
    "    if os.path.getsize(output_file) == 0:\n",
    "        writer.writerow([\n",
    "            'Hash', 'Message', 'Filename',\n",
    "            'Source Code (before)', 'Source Code (current)',\n",
    "            'Diff', 'LLM Inference (fix type)'\n",
    "        ])\n",
    "\n",
    "    counter = 0\n",
    "    count_comms = 0\n",
    "    count_comm_files = 0\n",
    "\n",
    "    try:\n",
    "        for commit in Repository(\n",
    "            repo_path,\n",
    "            only_in_branch='main',\n",
    "            only_modifications_with_file_types=['.py']\n",
    "        ).traverse_commits():\n",
    "            \n",
    "            msg_low = commit.msg.lower().strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "            if not any(term in msg_low for term in bug_terms):\n",
    "                continue\n",
    "\n",
    "            print(f\"Commit: {commit.hash} | {commit.msg[:60]}\")\n",
    "            count_comms += 1\n",
    "\n",
    "            for mf in commit.modified_files:\n",
    "                if count_comm_files >= 400:\n",
    "                    break\n",
    "\n",
    "                file_path = mf.new_path if mf.new_path else mf.old_path\n",
    "                if not file_path:\n",
    "                    continue\n",
    "\n",
    "                row_key = (commit.hash, file_path)\n",
    "                if row_key in processed_rows:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    before = (mf.source_code_before or \"\").replace('\\r', '\\\\r').replace('\\n', '\\\\n')\n",
    "                    after = (mf.source_code or \"\").replace('\\r', '\\\\r').replace('\\n', '\\\\n')\n",
    "                    diff_text = (mf.diff or \"\").replace('\\r', '\\\\r').replace('\\n', '\\\\n')\n",
    "\n",
    "                    # run LLM per diff hunk\n",
    "                    hunks = re.split(r'\\n@@ .* @@\\n', diff_text)\n",
    "                    new_comms = []\n",
    "                    for hunk in hunks:\n",
    "                        if not hunk.strip():\n",
    "                            continue\n",
    "                        input_text = commit.msg + \" diff \" + hunk\n",
    "                        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "                        output_ids = model.generate(\n",
    "                            input_ids, max_length=200, num_beams=5, early_stopping=True\n",
    "                        )\n",
    "                        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                        new_comms.append(output_text.replace('\\r', '\\\\r').replace('\\n', '\\\\n'))\n",
    "\n",
    "                        # memory cleanup\n",
    "                        del input_ids, output_ids\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                    writer.writerow([\n",
    "                        commit.hash, commit.msg.strip().replace(\"\\n\", \" \"),\n",
    "                        file_path, before, after, diff_text,\n",
    "                        \"; \".join(new_comms)\n",
    "                    ])\n",
    "                    counter += 1\n",
    "                    count_comm_files += 1\n",
    "\n",
    "                    if counter % 10 == 0:  # periodic progress\n",
    "                        f.flush()\n",
    "                        os.fsync(f.fileno())\n",
    "                        print(f\"--- {counter} rows processed and saved ---\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error on commit {commit.hash}, file {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if count_comm_files >= 400:\n",
    "                print(\"Reached 500 file changes, stopping.\")\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        print(\"Finished or interrupted. Progress saved.\")\n",
    "        print(count_comms, \"commits processed.\")\n",
    "        print(f\"A total of {count_comm_files} file change instances processed.\")\n",
    "        print(f\"Results written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a45dd245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51eb7ff1147641aaa13b5584e6ccb123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nupoo\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nupoo\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-4k-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bc994088de47389bf9033320e6eb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc20038e0d749da939a46d9cb537ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4dc3916a1e43c5af54d67d411a5fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a868570bc54358b548c048a9b72c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb680301f3604959b4dcf133afac736f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6fa6cdaeba4402b5b21ef8b4324166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845d3b7792584f16b25338a193736e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d70f06ff4b04146aea90c286d91a688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tokenizer2 \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/phi-3-mini-4k-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/phi-3-mini-4k-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_utils.py:3990\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3987\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[0;32m   3988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[0;32m   3989\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[1;32m-> 3990\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3999\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4001\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4002\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4003\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4006\u001b[0m     is_safetensors_available()\n\u001b[0;32m   4007\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m   4008\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4009\u001b[0m ):\n\u001b[0;32m   4010\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\utils\\hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[1;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    858\u001b[0m     )\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1009\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1543\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    454\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\", torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "133267df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"bug_commits_rectified.csv\"\n",
    "rectification_input = \"input_for_rectification.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding=\"utf-8\") as f_in, \\\n",
    "     open(rectification_input, 'w', newline='', encoding=\"utf-8\") as f_out:\n",
    "\n",
    "    reader = csv.DictReader(f_in)\n",
    "    writer = csv.writer(f_out)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    writer.writerow(['Hash', 'Filename', 'Message', 'LLM Inference (fix type)'])\n",
    "    \n",
    "    for row in reader:\n",
    "        writer.writerow([row['Hash'], row['Filename'], row['Message'], row['LLM Inference (fix type)']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09baf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"bug_commits_rectified.csv\"\n",
    "output_file = \"bug_commits_llm_rectified.csv\"\n",
    "\n",
    "with open(input_file, 'r', newline='', encoding=\"utf-8\") as f_in, \\\n",
    "     open(output_file, 'w', newline='', encoding=\"utf-8\") as f_out:\n",
    "    \n",
    "    reader = csv.DictReader(f_in)\n",
    "    fieldnames = reader.fieldnames + [\"Rectified Message\"]\n",
    "    writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for row in reader:\n",
    "        try:\n",
    "            # Construct the prompt for the second LLM\n",
    "            prompt = (\n",
    "                \"Rectify this commit message based on the code changes.\\n\"\n",
    "                f\"Message: {row['Message']}\\n\"\n",
    "                f\"Filename: {row['Filename']}\\n\"\n",
    "                f\"LLM Inference: {row['LLM Inference (fix type)']}\\n\"\n",
    "                \"Corrected commit message:\"\n",
    "            )\n",
    "            \n",
    "            input_ids = tokenizer2.encode(prompt, return_tensors=\"pt\")\n",
    "            output_ids = model2.generate(\n",
    "                input_ids, max_length=200, num_beams=5, early_stopping=True\n",
    "            )\n",
    "            rectified_msg = tokenizer2.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            row[\"Rectified Message\"] = rectified_msg\n",
    "            \n",
    "            # Clean up to save memory\n",
    "            del input_ids, output_ids\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {row['Hash']} - {row['Filename']}: {e}\")\n",
    "            row[\"Rectified Message\"] = \"\"\n",
    "        \n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Done! New CSV written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for rectification\n",
    "\n",
    "# tokenizer2 = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "# model2 = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\", torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
